---
title: "Microsoft Fabric EU Critical Raw Materials Learning Project"
title-block-banner: "#0275d8"
toc: True
---

# Summary

An end-to-end data analytics project in Microsoft Fabric (Data Factory & Power BI) using public statistics data from pdfs and APIs. The project aims to enhance existing ad-hoc analyses found in pdf-reports by connecting to new data sources.

The analyses aim to provide insights into risk of being dependent on and sourcing raw materials from a selection of different indicator categories, including 

- material criticality (from the EU Commissions pdf reports), 
- market development (from the Eurostat API), 
- price development (from Trading Economics API), and 
- environmental impact (from the Yale Environmental Performance Index, EPI).

# Project

In this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission's list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets. 

These two approaches enable additional *and* fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation ([EU CRM Regulation](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=OJ:L_202401252)).

:::{.callout-note}
EU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:

1. Economic Importance
2. Supply Risk

Classical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methdology document for how the indicators form the main categories [Link](https://op.europa.eu/en/publication-detail/-/publication/2d43b7e2-66ac-11e7-b2f2-01aa75ed71a1/language-en).
:::

## Importing to Data Factory

To start I open up a template for 'Basic data analytics' in Data Factory and name the workspace 'CRM Learning Project', which gives me just the necessary workloads I need for my analysis, namely:

- Data Factory for orchestrating 
- Data Warehouse for importing structured data
- Real-Time Intelligence to test out live price data, and
- Power BI for visualizations.

![Create a new Fabric workspace in Data Factory using template for basic data analytics](./images/DataFactory_basic_data_analytics_template.png)

#### Importing EU Critical Raw Materials Data

The data is currently published in pdfs on the [EU Commission's site for CRM](https://single-market-economy.ec.europa.eu/sectors/raw-materials/areas-specific-interest/critical-raw-materials_en). Using Excel's Power Query or Fabric's Data Factory to extract the data from the pdf's yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.

I explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a [public github repository](https://github.com/erikemilsson/EU-Critical-Raw-Materials-CRM-Reports-Datasets) (while of course following the EU Commission's licensing requirements). This

- simplifies the data ingestion stage considerably,
- enables other analysts and developers to use the datasets and collaborate, 
- creates transparency in data cleaning, and
- tracks updates for future updates of the list through version control.

![Github repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets](./images/github_repo.png)

To import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set 'Authentication kind' to *Anonymous* since the data is public.

:::{.callout-note}
To link to the csv files I used the direct Github link address structure: `https://raw.githubusercontent.com/<username>/<repository>/<branch>/<filepath>/<filename>.csv`
:::

During import I performed some simpler cleaning:

- In the 'Share'-column I removed rows with the value '<1%'.
- In the 'Share'-column I 
  - removed the percent signs with the 'Replace values' function, 
  - divided the values by 100 using the 
  - changed the datatype to 'Whole Number' temporarily so i can perform standard mathematical operations with the function under the 'Transform' tab, and
  - changed the datatype to percent.
- I replaced '-' values with null using the 'Replace values' function.
- I changed datatypes of decimals to "Decimal Number"
- I replaced 'PGMs', 'HREEs', and 'LREEs' values with 'PGM', 'HREE', 'LREE', respectively.

![Remove rows that contain '<1%'](./images/contains_<.png)

#### Importing Eurostat Data (tbc)

Import additional and/or updated statistics on a country-level.

#### Importing (tbc)

Import price data & market trends from e.g. Trading Economics API, World Bank Commodity Markets or IMF Primary Commodity Prices.

#### Importing Yale Environmental Performance Indicator (EPI) Data

Yale EPI data is published annually in csv files and can be found [here](https://epi.yale.edu/downloads). I opted to upload the files to my portfolio repository because they couldn't be uploaded directly to Fabric as it isn't tied to a OneDrive for Business account.

## Create a data warehouse

After collecting the data I create a data warehouse for my structured data and name it 'CRM_Data_Warehouse'. From here I can start creating schemas and store the data used for analytics in Power BI.

