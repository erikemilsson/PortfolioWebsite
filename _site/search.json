[
  {
    "objectID": "projects/project-1/project-1.html",
    "href": "projects/project-1/project-1.html",
    "title": "Project 1",
    "section": "",
    "text": "This project explores the relationship between sepal length and petal length in iris flowers.\nLink to Project Repository [invalid URL removed]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I specialize in cloud and data analytics to optimize logistics, supply chains, and sustainability strategies. Contact me on LinkedIn\n\nProfessional Projects\n\nOptimizing European Battery Recycling Through a Data-Driven Location and Technology Analysis\nTechnologies used: Python, Power BI, Github.\n\n\n\nLearning Projects\n\nMicrosoft Fabric End-to-End Analytics on Critical Raw Materials\nTechnologies used: Microsoft Fabric, Data Factory, Power BI, Github.\n\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nA script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I help businesses optimize their strategies with cloud and data solutions. My expertise lies in logistics and supply chains, sustainability reporting, and materials engineering. Explore my portfolio to see how I can help your business thrive."
  },
  {
    "objectID": "index.html#competency-overview",
    "href": "index.html#competency-overview",
    "title": "Erik Emilsson’s Portfolio Page",
    "section": "Competency Overview",
    "text": "Competency Overview\nI have over six year’s experience in sustainability management consulting in the product development & purchasing departments of a multi-national manufacturing conglomerate. In parallel, I have data analytics/science experience from several EU-funded research projects within electrification and circular economy. From my formal education in B.Sc chemical engineering (with engineering physics), and a M.Sc. applied physics I have a solid understanding of applied mathematical modelling and statistics.\nI’m a certified Power BI data analyst associate and I’m also certified in Azure cloud and Azure AI fundamentals. I’m also well underway to become certified in end-to-end enterprise analytics as a Fabric Analytics Engineer associate.\n\n\n“Innovation distinguishes between a leader and a follower.” - Steve Jobs\nSome of my favourite technologies and programming languages that I have used in my professional projects are:\n\nMicrosoft Power BI with DAX for enterprise dashboards.\nMicrosoft Excel with Power Query for calculations and ETL.\nPython for solving discrete optimization problems, ETL, and dynamic visuals.\nVS Code as my main code editor.\nMicrosoft Sharepoint and Teams for collaboration.\ndraw.io for data- and entity-modelling.\n\nI also have experience with the following from courses and personal projects:\n\nT-SQL in Microsoft SQL Server and Azure SQL Services for learning about data warehouseing and enterprise analytics.\nPostgreSQL for learning SQL for local data warehousing.\nData Warehousing with Standard SQL in GCP’s BiqQuery\nSqlite for learning SQL for Data Science\nR-language data cleaning, transformations, and visualizations.\nTableau for creating visuals in dashboards.\nPySpark for learning real-time enterprise analytics."
  },
  {
    "objectID": "index.html#professional-projects",
    "href": "index.html#professional-projects",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Professional Projects",
    "text": "Professional Projects\n\nEnd-of-Life Battery Recycling Facilty Location and Technology Selection with Reverse Logistics Optimization using Linear Programming\nGithub Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI\n\nBusiness Problem and Solution\nThe European Union is looking to scale up its battery recycling capacity in response to a burgeoning battery market in order to hit recycling targets. Several options for key technologies for recycling batteries are considered within the scope of the European market. The market is dynamic and there is much uncertainty with regards to how the projected increase in battery production for electrification will impact the region’s circular economy and carbon emission targets. The EU Commission wants:\n\nTo understand what battery producers will enter the market and the potential recycled battery material buyers in Europe, on both a geographical and aggregated level.\nTo optimize the geographical placement(s) and the recycling technology selection based on facility costs and CO2-emissions.\n\n\n\nMethod and Data\nBottom-up data is scraped from market actor’s published volumes of batteries and their geograhical coordinates. Open-source material data from Argonne National Laboratories and basic chemistry calculations are used to linearly extrapolate the volumes between different battery chemistries and applications to a common functional unit.\nFor facility cost data, another engineer’s calculations of a technoeconomic assessment by country is used. For facility CO2-emission data, an LCA is done my LCA-practitioners in the same project and used in the optimization model.\nData processing and optimization is done in Python (Pandas and PuLP). Visualizations for analytical purposes are done using Python’s Plotly package and visualizations for stakeholders are done with Power BI.\n\n\nResults\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region."
  },
  {
    "objectID": "index.html#personal-projects",
    "href": "index.html#personal-projects",
    "title": "Erik Emilsson’s Portfolio Page",
    "section": "Personal Projects",
    "text": "Personal Projects\nUnder construction.\n\n\n\n\nA problem well-stated is a problem half-solved. - Charles F. Kettering"
  },
  {
    "objectID": "projects/projects.html#professional-projects",
    "href": "projects/projects.html#professional-projects",
    "title": "Projects",
    "section": "Professional Projects",
    "text": "Professional Projects\nUnder construction.\n\nELiMINATE\n\n\n\nResults from linear programming."
  },
  {
    "objectID": "projects/projects.html#personal-projects",
    "href": "projects/projects.html#personal-projects",
    "title": "Projects",
    "section": "Personal Projects",
    "text": "Personal Projects\nUnder construction.\n\n\n\n\nA problem well-stated is a problem half-solved. - Charles F. Kettering"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "ELiMINATE\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python.\n\n\n\nCase Study How Does a Bike-Share Navigate Speedy Success\n\n\n\n\nPlastics in Passenger Cars\n\n\nAttitudes towards battery recycling in Sweden\n\n\n\n\nA problem well-stated is a problem half-solved. - Charles F. Kettering"
  },
  {
    "objectID": "projects/projects.html#eliminate",
    "href": "projects/projects.html#eliminate",
    "title": "Projects",
    "section": "ELiMINATE",
    "text": "ELiMINATE\n\n\n\nResults from linear programming."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I have over six year’s experience in sustainability management consulting in the product development & purchasing departments of a multi-national manufacturing conglomerate. In parallel, I have data analytics/science experience from several EU-funded research projects within electrification and circular economy. From my formal education in B.Sc chemical engineering (with engineering physics), and a M.Sc. applied physics I have a solid understanding of applied mathematical modelling and statistics.\nI’m a certified Power BI Data Analyst associate and I’m also certified in Azure cloud and Azure AI fundamentals. I’m also well underway to become certified in end-to-end enterprise analytics as a Fabric Analytics Engineer associate.\n\n\n“Innovation distinguishes between a leader and a follower.” - Steve Jobs\nSome of my favourite technologies and programming languages that I have used in my professional projects are:\n\nMicrosoft Power BI with DAX for enterprise dashboards.\nMicrosoft Excel with Power Query for calculations and ETL.\nPython for solving discrete optimization problems, ETL, and dynamic visuals.\nVS Code as my main code editor.\nMicrosoft Sharepoint and Teams for collaboration.\ndraw.io for data- and entity-modelling.\n\nI also have experience with the following from courses and personal projects:\n\nT-SQL in Microsoft SQL Server and Azure SQL Services for learning about data warehouseing and enterprise analytics.\nPostgreSQL for learning SQL for local data warehousing.\nData Warehousing with Standard SQL in GCP’s BiqQuery\nSqlite for learning SQL for Data Science\nR-language data cleaning, transformations, and visualizations.\nTableau for creating visuals in dashboards.\nPySpark for learning real-time enterprise analytics.\n\nI’m currently living in Gothenburg Sweden and I’ve previously lived in\n\nMalaga, Spain\nKalmar, Sweden\nTokyo, Japan\nLondon, England\nHouston, USA\n\n\n\n\n\n\nTjörn, Sweden\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa"
  },
  {
    "objectID": "projects/professional_projects.html",
    "href": "projects/professional_projects.html",
    "title": "Projects",
    "section": "",
    "text": "This page is still under construction.\n\nELiMINATE\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "projects/learning_projects.html",
    "href": "projects/learning_projects.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Here I share some of my learning projects that I’ve done to gain experience in SQL, cloud services, data warehousing, and more.\n\nLearning Projects\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nBelow is the script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "about.html#competency-overview",
    "href": "about.html#competency-overview",
    "title": "More About Me",
    "section": "",
    "text": "I have over six year’s experience in sustainability management consulting in the product development & purchasing departments of a multi-national manufacturing conglomerate. In parallel, I have data analytics/science experience from several EU-funded research projects within electrification and circular economy. From my formal education in B.Sc chemical engineering (with engineering physics), and a M.Sc. applied physics I have a solid understanding of applied mathematical modelling and statistics.\nI’m a certified Power BI Data Analyst associate and I’m also certified in Azure cloud and Azure AI fundamentals. I’m also well underway to become certified in end-to-end enterprise analytics as a Fabric Analytics Engineer associate.\n\n\n“Innovation distinguishes between a leader and a follower.” - Steve Jobs\nSome of my favourite technologies and programming languages that I have used in my professional projects are:\n\nMicrosoft Power BI with DAX for enterprise dashboards.\nMicrosoft Excel with Power Query for calculations and ETL.\nPython for solving discrete optimization problems, ETL, and dynamic visuals.\nVS Code as my main code editor.\nMicrosoft Sharepoint and Teams for collaboration.\ndraw.io for data- and entity-modelling.\n\nI also have experience with the following from courses and personal projects:\n\nT-SQL in Microsoft SQL Server and Azure SQL Services for learning about data warehouseing and enterprise analytics.\nPostgreSQL for learning SQL for local data warehousing.\nData Warehousing with Standard SQL in GCP’s BiqQuery\nSqlite for learning SQL for Data Science\nR-language data cleaning, transformations, and visualizations.\nTableau for creating visuals in dashboards.\nPySpark for learning real-time enterprise analytics."
  },
  {
    "objectID": "about.html#more-about-me",
    "href": "about.html#more-about-me",
    "title": "More About Me",
    "section": "More about me",
    "text": "More about me\nLiving in Gothenburg Sweden. I’ve previously lived in\n\nMalaga, Spain\nKalmar, Sweden\nTokyo, Japan\nLondon, England\nHouston, USA"
  },
  {
    "objectID": "index.html#eliminate",
    "href": "index.html#eliminate",
    "title": "Erik Emilsson’s Portfolio",
    "section": "ELiMINATE",
    "text": "ELiMINATE\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#mapping-of-a-european-battery-supply-chain",
    "href": "index.html#mapping-of-a-european-battery-supply-chain",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Mapping of a European battery supply chain",
    "text": "Mapping of a European battery supply chain\n\nBusiness Problem\nSeveral technologies for recycling batteries are considered within the scope of the European market. The market is dynamic, with a burgeoning number of actors opening up or increasing their production capacity in the coming years.\n\nMap the battery producers and the potential recycled battery material buyers in Europe to understand the dynamics from 2022 to 2030.\nOptimize the geographical placement(s) and the technology selection based on facility costs and CO2-emissions.\n\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#reverse-logistics-optimization-of-a-european-battery-supply-chain",
    "href": "index.html#reverse-logistics-optimization-of-a-european-battery-supply-chain",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Reverse Logistics Optimization of a European battery supply chain",
    "text": "Reverse Logistics Optimization of a European battery supply chain\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#mapping-and-reverse-logistics-optimization-of-a-battery-recycler-facility-in-the-european-battery-supply-chain",
    "href": "index.html#mapping-and-reverse-logistics-optimization-of-a-battery-recycler-facility-in-the-european-battery-supply-chain",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Mapping and Reverse Logistics Optimization of a Battery Recycler Facility in the European Battery Supply Chain",
    "text": "Mapping and Reverse Logistics Optimization of a Battery Recycler Facility in the European Battery Supply Chain\n\nBusiness Problem\nThe European Union is looking to scale up its battery recycling capacity to hit circular economy and carbon emission targets. Several technologies for recycling batteries are considered within the scope of the European market. The market is dynamic, with a burgeoning number of actors opening up or increasing their production capacity in the coming years.\n\nMap the battery producers and the potential recycled battery material buyers in Europe to understand the dynamics from 2022 to 2030.\nOptimize the geographical placement(s) and the technology selection based on facility costs and CO2-emissions.\n\n\n\nEuropean Battery Market Mapping\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard of lithium-ion batteries in Europe created in Power BI.\n\n\n\nOptimization Results\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#optimization-results",
    "href": "index.html#optimization-results",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Optimization Results",
    "text": "Optimization Results\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#optimize-flows-and-technology-selection-of-end-of-life-batteries-using-reverse-logistics-optimization",
    "href": "index.html#optimize-flows-and-technology-selection-of-end-of-life-batteries-using-reverse-logistics-optimization",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Optimize Flows and Technology Selection of End-of-Life Batteries Using Reverse Logistics Optimization",
    "text": "Optimize Flows and Technology Selection of End-of-Life Batteries Using Reverse Logistics Optimization\n\nBusiness Problem and Solution\nThe European Union is looking to scale up its battery recycling capacity to hit recycling targets. Several options for key technologies for recycling batteries are considered within the scope of the European market. The market is dynamic, with a burgeoning number of actors opening up or increasing their production capacity in the coming years. The EU Commission wants to know:\n\nMap the battery producers and the potential recycled battery material buyers in Europe to understand the dynamics from 2022 to 2030, on both a geographical and aggregated level.\nOptimize the geographical placement(s) and the recycling technology selection based on facility costs and CO2-emissions.\n\n\n\nData Availability and Limitations\nThere is bottom-up and top-down data for\n\n\nEuropean Battery Market Mapping\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\n\n\nOptimization Results\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard gives users the ability to explore how the battery recycling market is expected to change for the years 2022, 2026, and 2030.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python."
  },
  {
    "objectID": "index.html#optimize-placement-and-technology-selection-of-end-of-life-batteries-using-reverse-logistics-optimization",
    "href": "index.html#optimize-placement-and-technology-selection-of-end-of-life-batteries-using-reverse-logistics-optimization",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Optimize Placement and Technology Selection of End-of-Life Batteries Using Reverse Logistics Optimization",
    "text": "Optimize Placement and Technology Selection of End-of-Life Batteries Using Reverse Logistics Optimization\n\nBusiness Problem and Solution\nThe European Union is looking to scale up its battery recycling capacity in response to a burgeoning battery market in order to hit recycling targets. Several options for key technologies for recycling batteries are considered within the scope of the European market. The market is dynamic and there is much uncertainty with regards to how the projected increase in battery production for electrification will impact the region’s circular economy and carbon emission targets. The EU Commission wants:\n\nTo understand what battery producers will enter the market and the potential recycled battery material buyers in Europe, on both a geographical and aggregated level.\nTo optimize the geographical placement(s) and the recycling technology selection based on facility costs and CO2-emissions.\n\n\n\nMethod and Data\nBottom-up data is scraped from market actor’s published volumes of batteries and their geograhical coordinates. Open-source material data from Argonne National Laboratories and basic chemistry calculations are used to linearly extrapolate the volumes between different battery chemistries and applications to a common functional unit.\nFor facility cost data, another engineer’s calculations of a technoeconomic assessment by country is used. For facility CO2-emission data, an LCA is done my LCA-practitioners in the same project and used in the optimization model.\nData processing and optimization is done in Python (Pandas and PuLP). Visualizations for analytical purposes are done using Python’s Plotly package and visualizations for stakeholders are done with Power BI.\n\n\nResults\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard gives users the ability to explore how the battery recycling market is expected to change for the years 2022, 2026, and 2030.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python.\n\nSee the Github Repo and EU-funded Project Report and Project Page"
  },
  {
    "objectID": "projects/learning_projects.html#case-study-how-does-a-bike-share-navigate-speedy-success",
    "href": "projects/learning_projects.html#case-study-how-does-a-bike-share-navigate-speedy-success",
    "title": "Learning Projects",
    "section": "Case Study How Does a Bike-Share Navigate Speedy Success",
    "text": "Case Study How Does a Bike-Share Navigate Speedy Success\nTechnologies used: BigQuery Data Warehouse, R-language.\nScript from the Google Data Analytics Capstone project utilizing SQL in Bigquery’s sandbox environment and visualizations in R-language."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Projects",
    "text": "Projects\n\nOptimize Placement and Technology Selection of End-of-Life Batteries Using Reverse Logistics Optimization\n\nBusiness Problem and Solution\nThe European Union is looking to scale up its battery recycling capacity in response to a burgeoning battery market in order to hit recycling targets. Several options for key technologies for recycling batteries are considered within the scope of the European market. The market is dynamic and there is much uncertainty with regards to how the projected increase in battery production for electrification will impact the region’s circular economy and carbon emission targets. The EU Commission wants:\n\nTo understand what battery producers will enter the market and the potential recycled battery material buyers in Europe, on both a geographical and aggregated level.\nTo optimize the geographical placement(s) and the recycling technology selection based on facility costs and CO2-emissions.\n\n\n\nMethod and Data\nBottom-up data is scraped from market actor’s published volumes of batteries and their geograhical coordinates. Open-source material data from Argonne National Laboratories and basic chemistry calculations are used to linearly extrapolate the volumes between different battery chemistries and applications to a common functional unit.\nFor facility cost data, another engineer’s calculations of a technoeconomic assessment by country is used. For facility CO2-emission data, an LCA is done my LCA-practitioners in the same project and used in the optimization model.\nData processing and optimization is done in Python (Pandas and PuLP). Visualizations for analytical purposes are done using Python’s Plotly package and visualizations for stakeholders are done with Power BI.\n\n\nResults\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nProjected resource flows for battery recycling in Europe in 2030.\n\n\nProjected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe\n\n\nA Power BI dashboard gives users the ability to explore how the battery recycling market is expected to change for the years 2022, 2026, and 2030.\n\nThe optimal network for lithium-ion battery recycling in Europe for different recycling technologies using linear programming in Python.\n\nSee the Github Repo and EU-funded Project Report and Project Page"
  },
  {
    "objectID": "projects/learning_projects.html#professional-projects",
    "href": "projects/learning_projects.html#professional-projects",
    "title": "Learning Projects",
    "section": "",
    "text": "Here I share some of my learning projects that I’ve done to gain experience in cloud and data services."
  },
  {
    "objectID": "projects/learning_projects.html#bike-share-business-analsis-for-adopting-more-members",
    "href": "projects/learning_projects.html#bike-share-business-analsis-for-adopting-more-members",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Bike-Share Business Analsis for Adopting More Members",
    "text": "Bike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nBelow is the script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "index.html#battery-recycling-facility-optimization-reverse-logistics-linear-programming",
    "href": "index.html#battery-recycling-facility-optimization-reverse-logistics-linear-programming",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Battery Recycling Facility Optimization: Reverse Logistics & Linear Programming",
    "text": "Battery Recycling Facility Optimization: Reverse Logistics & Linear Programming\nGithub Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\nMethod and Data\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\nResults\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "index.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "href": "index.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "title": "Erik Emilsson’s Portfolio",
    "section": "Optimizing European Battery Recycling Through a Data-Driven Location and Technology Analysis",
    "text": "Optimizing European Battery Recycling Through a Data-Driven Location and Technology Analysis\nGithub Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\nMethod and Data\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\nResults\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "about.html#pictures",
    "href": "about.html#pictures",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Tjörn, Sweden\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa"
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "",
    "text": "An end-to-end data analytics project in Microsoft Fabric (Data Factory & Power BI) using public statistics data from pdfs and APIs. The project aims to enhance existing ad-hoc analyses found in pdf-reports by connecting to new data sources.\nThe analyses aim to provide insights into risk of being dependent on and sourcing raw materials from a selection of different indicator categories, including\n\nmaterial criticality (from the EU Commissions pdf reports),\nmarket development (from the Eurostat API),\nprice development (from Trading Economics API), and\nenvironmental impact (from the Yale Environmental Performance Index, EPI)."
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#method-and-data",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#method-and-data",
    "title": "Microsoft Fabric Critical Raw Materials Learning Project",
    "section": "Method and Data",
    "text": "Method and Data\nEU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Power Query in Microsoft Excel to extract the data from those pages yields unsatisfactory results. I opted to get better control of the data by using a Github repository, which also gives me the option to track changes in updates."
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-import",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-import",
    "title": "Microsoft Fabric Critical Raw Materials Learning Project",
    "section": "Datasets import",
    "text": "Datasets import\nEU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with several errors. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nThe size of the datasets are relatively small and manageable. To solve this issue of ingesting data to my pipeline, I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This also enables other analysts and developers to use the datasets, and tracks updates for future updates of the list."
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#understanding-the-datasets",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#understanding-the-datasets",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Understanding the Datasets",
    "text": "Understanding the Datasets\nEU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in data cleaning, and\ntracks updates for future updates of the list through version control.\n\nEurostat Data"
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-overview-and-import-to-data-factory",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-overview-and-import-to-data-factory",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Datasets Overview and Import to Data Factory",
    "text": "Datasets Overview and Import to Data Factory\n1. Pre-Cleaning EU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in datfa cleaning, and\ntracks updates for future updates of the list through version control.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\n2. Opening up Data Factory\n3. Importing Eurostat Data"
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-overview",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#datasets-overview",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Datasets Overview",
    "text": "Datasets Overview\n1. Pre-Cleaning EU Critical Raw Materials Data"
  },
  {
    "objectID": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#importing-to-data-factory",
    "href": "projects/Fabric CRM Learning Project/Fabric CRM Learning Project.html#importing-to-data-factory",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Importing to Data Factory",
    "text": "Importing to Data Factory\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\nImporting EU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in data cleaning, and\ntracks updates for future updates of the list through version control.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100 using the\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\nImporting Eurostat Data"
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "",
    "text": "An end-to-end data analytics project in Microsoft Fabric (Data Factory & Power BI) using public statistics data from pdfs and APIs. The project aims to enhance existing ad-hoc analyses found in pdf-reports by connecting to new data sources.\nThe analyses aim to provide insights into risk of being dependent on and sourcing raw materials from a selection of different indicator categories, including\n\nmaterial criticality (from the EU Commissions pdf reports),\nmarket development (from the Eurostat API),\nprice development (from Trading Economics API), and\nenvironmental impact (from the Yale Environmental Performance Index, EPI)."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html#importing-to-data-factory",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html#importing-to-data-factory",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Importing to Data Factory",
    "text": "Importing to Data Factory\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\nImporting EU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in data cleaning, and\ntracks updates for future updates of the list through version control.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100 using the\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\nImporting Eurostat Data (tbc)\nImport additional and/or updated statistics on a country-level.\n\n\nImporting (tbc)\nImport price data & market trends from e.g. Trading Economics API, World Bank Commodity Markets or IMF Primary Commodity Prices.\n\n\nImporting Yale Environmental Performance Indicator (EPI) Data\nYale EPI data is published annually in csv files and can be found here. I opted to upload the files to my portfolio repository because they couldn’t be uploaded directly to Fabric as it isn’t tied to a OneDrive for Business account."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html#create-a-data-warehouse",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric CRM Learning Project.html#create-a-data-warehouse",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Create a data warehouse",
    "text": "Create a data warehouse\nAfter collecting the data I create a data warehouse for my structured data and name it ‘CRM_Data_Warehouse’. From here I can start creating schemas and store the data used for analytics in Power BI."
  },
  {
    "objectID": "projects/Eliminate/eliminate.html",
    "href": "projects/Eliminate/eliminate.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "href": "projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "other_projects/project-1/project-1.html",
    "href": "other_projects/project-1/project-1.html",
    "title": "Project 1",
    "section": "",
    "text": "This project explores the relationship between sepal length and petal length in iris flowers.\nLink to Project Repository [invalid URL removed]"
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?\n\n\n\n\nIn this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses. \n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.  #### Saving to historical exchange rate to datalake (tbc)"
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#importing-to-data-factory",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#importing-to-data-factory",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Importing to Data Factory",
    "text": "Importing to Data Factory\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\nImporting EU Critical Raw Materials Data\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in data cleaning, and\ntracks updates for future updates of the list through version control.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100 using the\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\nImporting Eurostat Data (tbc)\nImport additional and/or updated statistics on a country-level.\n\n\nImporting (tbc)\nImport price data & market trends from e.g. Trading Economics API, World Bank Commodity Markets or IMF Primary Commodity Prices.\n\n\nImporting Yale Environmental Performance Indicator (EPI) Data\nYale EPI data is published annually in csv files and can be found here. I opted to upload the files to my portfolio repository because they couldn’t be uploaded directly to Fabric as it isn’t tied to a OneDrive for Business account."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#create-a-data-warehouse",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#create-a-data-warehouse",
    "title": "Microsoft Fabric EU Critical Raw Materials Learning Project",
    "section": "Create a data warehouse",
    "text": "Create a data warehouse\nAfter collecting the data I create a data warehouse for my structured data and name it ‘CRM_Data_Warehouse’. From here I can start creating schemas and store the data used for analytics in Power BI."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#summary",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#summary",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "An end-to-end data analytics project in Microsoft Fabric (Data Factory & Power BI) using public statistics data from pdfs and APIs. The project aims to enhance existing ad-hoc analyses found in pdf-reports by connecting to new data sources.\nThe analyses aim to provide insights into risk of being dependent on and sourcing raw materials from a selection of different indicator categories, including\n\nmaterial criticality (from the EU Commissions pdf reports),\nmarket development (from the Eurostat API),\nprice development (from Trading Economics API), and\nenvironmental impact (from the Yale Environmental Performance Index, EPI)."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "In this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methdology document for how the indicators form the main categories Link.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nThe data is currently published in pdfs on the EU Commission’s site for CRM. Using Excel’s Power Query or Fabric’s Data Factory to extract the data from the pdf’s yields data with critical errors in the dataframe structures, e.g. mismatching columns and cell values. Additionally, the data is spread over several years and is not standardized, making future updates difficult to automate.\nI explored the datasets while considering ways to effectively manage the data cleaning. The size of the datasets are relatively small and manageable, so I opted to get better control of the source data by combining Power Query with manual cleaning in Excel, and publishing to a public github repository (while of course following the EU Commission’s licensing requirements). This:\n\nsimplifies the data ingestion stage considerably,\nenables other analysts and developers to use the datasets and collaborate,\ncreates transparency in data cleaning, and\ntracks updates for future updates of the list through version control.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100 using the\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model.\n\n\n\nImport price data & market trends from e.g. Trading Economics API, World Bank Commodity Markets or IMF Primary Commodity Prices.\n\n\n\nYale EPI data is published annually in csv files and can be found here. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account.\n\n\n\n\nAfter collecting the data I create a data warehouse for my structured data and name it ‘CRM_Data_Warehouse’. From here I can start creating schemas and store the data used for analytics in Power BI."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?"
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "In this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses. \n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.  #### Saving to historical exchange rate to datalake (tbc)"
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html",
    "href": "Data Engineering Portfolio Project Plan.html",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Identify 2-3 industries that genuinely interest me (e.g., e-commerce, streaming, gaming, finance). Research the common types of data and challenges within those industries (e.g., user clickstreams, sales transactions, game telemetry) using LLMs, industry reports, job postings, and/or data engineering blogs. Choose a project theme based on a common, relatable dataset type that allows you to demonstrate a wide range of DE skills. This still shows industry awareness without risking analysis paralysis.\n\n\n\n\nRealistic: Not perfectly clean; requires cleaning, handling missing values, different formats (CSV, JSON, etc.).\nSufficiently Complex: Allows for joins, aggregations, and meaningful transformations.\nAppropriately Sized: Manageable for a personal project but large enough to be non-trivial (millions of rows).\nPublicly Available: E.g. Kaggle, government open data, Awesome Public Datasets on GitHub.\n\n\n\n\nDefining who the data is for (e.g., marketing team, sales analysts) and what they need to do with it (e.g., analyze campaign effectiveness, track regional sales) to identify the purpose for the pipeline. It will guide technical decisions regarding transformations, data models, and the final dashboard.\n\n\n\nUsing Fabric components like Lakehouse, Notebooks, Dataflows/Pipelines, semantic models, BI-dashboards, etc.. The medallion architecture and final dashboards should clearly address the business problems / use-cases defined in Step 3. ## Step 5: Connect to GitHub & Set Up Repository - A clear, detailed README.md explaining the project, architecture, setup, and how to view the results. - Logical code organization. - Good commit hygiene. ## Step 6: Set up CI/CD in Microsoft Fabric Demonstrate CI/CD concepts using Deployment Pipelines for supported artifacts (e.g., Power BI reports), and simulate or document manual deployment steps for unsupported components.\n\n\n\n\nExecutive summary, with a clearly defined business problem the solution seeks to fix.\nA complete list of data sources\nThe purpose of the dataset\nHow data integrity and quality were maintained\nWorkspace assignment and reasoning.\nSnapshots of:\n\nthe dashboards for the different users\ndata lineage, and\nthe data pipeline (including a text summary of the ETL-automation steps)\n\n\n\n\n\n\nSince the Fabric trial will run out eventually, migrate the gold-level semantic models to Power BI desktop, publish to Power BI Service and then select “File”&gt; “Embed report” &gt; “Publish to web (public)” to get a link that I can use to share my dashboard for free (alongside all underlying data, but that is not a problem as the datasets will be public anyway).\nUsing Power BI Free and publishing publicly makes it easy to share in your portfolio.\n\n\n\n\n\nThe documented project summary\nThe embedded Power BI report\nLinks to the Github repository\n\nAdditionally, create a youtube walkthrough videos and embed the video in my portfolio."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-1-identify-target-companies-data-needs",
    "href": "Data Engineering Portfolio Project Plan.html#step-1-identify-target-companies-data-needs",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Identify 2-3 industries that genuinely interest me (e.g., e-commerce, streaming, gaming, finance). Research the common types of data and challenges within those industries (e.g., user clickstreams, sales transactions, game telemetry) using LLMs, industry reports, job postings, and/or data engineering blogs. Choose a project theme based on a common, relatable dataset type that allows you to demonstrate a wide range of DE skills. This still shows industry awareness without risking analysis paralysis."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-2-find-suitable-datasets-online",
    "href": "Data Engineering Portfolio Project Plan.html#step-2-find-suitable-datasets-online",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Realistic: Not perfectly clean; requires cleaning, handling missing values, different formats (CSV, JSON, etc.).\nSufficiently Complex: Allows for joins, aggregations, and meaningful transformations.\nAppropriately Sized: Manageable for a personal project but large enough to be non-trivial (millions of rows).\nPublicly Available: E.g. Kaggle, government open data, Awesome Public Datasets on GitHub."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-3-define-end-users-use-cases",
    "href": "Data Engineering Portfolio Project Plan.html#step-3-define-end-users-use-cases",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Defining who the data is for (e.g., marketing team, sales analysts) and what they need to do with it (e.g., analyze campaign effectiveness, track regional sales) to identify the purpose for the pipeline. It will guide technical decisions regarding transformations, data models, and the final dashboard."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-4-set-up-workspaces-create-resources-implement-pipeline-with-medallion-architecture-bronzesilvergold",
    "href": "Data Engineering Portfolio Project Plan.html#step-4-set-up-workspaces-create-resources-implement-pipeline-with-medallion-architecture-bronzesilvergold",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Using Fabric components like Lakehouse, Notebooks, Dataflows/Pipelines, semantic models, BI-dashboards, etc.. The medallion architecture and final dashboards should clearly address the business problems / use-cases defined in Step 3. ## Step 5: Connect to GitHub & Set Up Repository - A clear, detailed README.md explaining the project, architecture, setup, and how to view the results. - Logical code organization. - Good commit hygiene. ## Step 6: Set up CI/CD in Microsoft Fabric Demonstrate CI/CD concepts using Deployment Pipelines for supported artifacts (e.g., Power BI reports), and simulate or document manual deployment steps for unsupported components."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-7-document-the-final-project",
    "href": "Data Engineering Portfolio Project Plan.html#step-7-document-the-final-project",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Executive summary, with a clearly defined business problem the solution seeks to fix.\nA complete list of data sources\nThe purpose of the dataset\nHow data integrity and quality were maintained\nWorkspace assignment and reasoning.\nSnapshots of:\n\nthe dashboards for the different users\ndata lineage, and\nthe data pipeline (including a text summary of the ETL-automation steps)"
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-8-create-dashboard-power-bi-freepublic",
    "href": "Data Engineering Portfolio Project Plan.html#step-8-create-dashboard-power-bi-freepublic",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "Since the Fabric trial will run out eventually, migrate the gold-level semantic models to Power BI desktop, publish to Power BI Service and then select “File”&gt; “Embed report” &gt; “Publish to web (public)” to get a link that I can use to share my dashboard for free (alongside all underlying data, but that is not a problem as the datasets will be public anyway).\nUsing Power BI Free and publishing publicly makes it easy to share in your portfolio."
  },
  {
    "objectID": "Data Engineering Portfolio Project Plan.html#step-9-publish-the-entire-project-in-my-portfolio-website-including-youtube-walkthrough",
    "href": "Data Engineering Portfolio Project Plan.html#step-9-publish-the-entire-project-in-my-portfolio-website-including-youtube-walkthrough",
    "title": "Data Engineering Portfolio Project Plan",
    "section": "",
    "text": "The documented project summary\nThe embedded Power BI report\nLinks to the Github repository\n\nAdditionally, create a youtube walkthrough videos and embed the video in my portfolio."
  },
  {
    "objectID": "Journal.html",
    "href": "Journal.html",
    "title": "Project Journal",
    "section": "",
    "text": "In this journal I track my progress in this Fabric project. Although it is mainly for myself to keep tabs on what I’ve done, I’m sharing it here in case it is of use to anyone looking in.\n\n\n\n\nA few days ago I created this github repository and set up Fabric to use it for version control. It required going into the Admin Portal to allow the Github to be used. It wasn’t particularly difficult to follow the Microsoft Learn documentation.\nI also created two separate workspaces for what I foresee could be two discrete use-cases for analytics on manufacturing, critical raw materials, and pricing. I have some general idea for what I want to create, but I’m also flexible on changing direction if I find that something else makes more sense. The two workspaces are:\n\nrisk&insights_team workspace: Meant for data engineers and data analysts who set up the data pipelines and design dashboards for reporting and analysis.\ncommunication_team workspace: Meant almost solely for end-users of the data, with more restrictions on how they’re allowed to use the data.\n\nI also started thinking of what type of data sources I could bring into Fabric to get valuable insights related to the sourcing of materials for e.g. an electronics manufacturer. There are many choices. Using an LLM, I get a few options:\n\nBill of Materials (BOM) Database – Stores structured information on components, materials, and subassemblies required for each product\nProduct Lifecycle Management (PLM) Database – Tracks product versions, engineering changes, and component obsolescence.\nSupplier & Vendor Database – Contains information on approved suppliers, material costs, and sourcing history.\nEnterprise Resource Planning (ERP) Database – Centralizes purchasing, financials, and supply chain data, often integrating with other databases.\n\nI’ll look into what attributes each database might have at a later stage. For now it is good that I did some research into this, so I know I’m on the right path. My aim is to later create one or more databases using synthetic data, using SQL server on my local desktop PC to mimic an on-premises database. I plan to use a data gateway so I can create a secure connection to Fabric.\nApart from the data that a manufacturer keeps on-premises, I also want to connect the data to some live data. I first considered creating my own, with sensors, or using an iterating Python script. However, since I couldn’t envision them adding any additional value to the reports and visualizations I was going to create, I scrapped both ideas.\nThen I started looking into the possiblity of using raw material price data to get live data into Fabric using an EventStream. My inital thought was to create a web scraping script with Python, store the snapshots in SQL Server locally, and creating a Data Gateway and expose it to Fabric. However, the terms of use in most services that provide price don’t allow for it, so I decided to look for an API and do more research on possible sources.\n\n\n\nToday I looked over several sources that provide commodity (metals and minerals) data with APIs that I can load into a Fabric Eventhouse. after looking around at various sources I found one that was called MetalPriceAPI that has a free-tier (that provides data in 30-minute intervals), and a basic-tier for about 100SEK (which provides data in 10-minute intervals). It has several precious metals and base metals, and also prices in many currencies. I will get started with the free-tier, and then upgrade once everything else is built and connected.\n\n\n\n\nI looked through the MetalPriceAPI documentaton. It’s an external HTTP API, so it can’t connect to a Fabric Eventstream directly as it isn’t listed as one a type of source it can connect to."
  },
  {
    "objectID": "Journal.html#section",
    "href": "Journal.html#section",
    "title": "Project Journal",
    "section": "",
    "text": "A few days ago I created this github repository and set up Fabric to use it for version control. It required going into the Admin Portal to allow the Github to be used. It wasn’t particularly difficult to follow the Microsoft Learn documentation.\nI also created two separate workspaces for what I foresee could be two discrete use-cases for analytics on manufacturing, critical raw materials, and pricing. I have some general idea for what I want to create, but I’m also flexible on changing direction if I find that something else makes more sense. The two workspaces are:\n\nrisk&insights_team workspace: Meant for data engineers and data analysts who set up the data pipelines and design dashboards for reporting and analysis.\ncommunication_team workspace: Meant almost solely for end-users of the data, with more restrictions on how they’re allowed to use the data.\n\nI also started thinking of what type of data sources I could bring into Fabric to get valuable insights related to the sourcing of materials for e.g. an electronics manufacturer. There are many choices. Using an LLM, I get a few options:\n\nBill of Materials (BOM) Database – Stores structured information on components, materials, and subassemblies required for each product\nProduct Lifecycle Management (PLM) Database – Tracks product versions, engineering changes, and component obsolescence.\nSupplier & Vendor Database – Contains information on approved suppliers, material costs, and sourcing history.\nEnterprise Resource Planning (ERP) Database – Centralizes purchasing, financials, and supply chain data, often integrating with other databases.\n\nI’ll look into what attributes each database might have at a later stage. For now it is good that I did some research into this, so I know I’m on the right path. My aim is to later create one or more databases using synthetic data, using SQL server on my local desktop PC to mimic an on-premises database. I plan to use a data gateway so I can create a secure connection to Fabric.\nApart from the data that a manufacturer keeps on-premises, I also want to connect the data to some live data. I first considered creating my own, with sensors, or using an iterating Python script. However, since I couldn’t envision them adding any additional value to the reports and visualizations I was going to create, I scrapped both ideas.\nThen I started looking into the possiblity of using raw material price data to get live data into Fabric using an EventStream. My inital thought was to create a web scraping script with Python, store the snapshots in SQL Server locally, and creating a Data Gateway and expose it to Fabric. However, the terms of use in most services that provide price don’t allow for it, so I decided to look for an API and do more research on possible sources.\n\n\n\nToday I looked over several sources that provide commodity (metals and minerals) data with APIs that I can load into a Fabric Eventhouse. after looking around at various sources I found one that was called MetalPriceAPI that has a free-tier (that provides data in 30-minute intervals), and a basic-tier for about 100SEK (which provides data in 10-minute intervals). It has several precious metals and base metals, and also prices in many currencies. I will get started with the free-tier, and then upgrade once everything else is built and connected."
  },
  {
    "objectID": "Journal.html#section-1",
    "href": "Journal.html#section-1",
    "title": "Project Journal",
    "section": "",
    "text": "I looked through the MetalPriceAPI documentaton. It’s an external HTTP API, so it can’t connect to a Fabric Eventstream directly as it isn’t listed as one a type of source it can connect to."
  }
]