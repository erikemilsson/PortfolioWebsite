[
  {
    "objectID": "PortfolioWebsite/about.html",
    "href": "PortfolioWebsite/about.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I have over six year’s experience in sustainability management consulting in the product development & purchasing departments of a multi-national manufacturing conglomerate. In parallel, I have data analytics/science experience from several EU-funded research projects within electrification and circular economy. From my formal education in B.Sc chemical engineering (with engineering physics), and a M.Sc. applied physics I have a solid understanding of applied mathematical modelling and statistics.\nI’m a certified Power BI Data Analyst associate and I’m also certified in Azure cloud and Azure AI fundamentals. I’m also well underway to become certified in end-to-end enterprise analytics as a Fabric Analytics Engineer associate.\n\n\n“Innovation distinguishes between a leader and a follower.” - Steve Jobs\nSome of my favourite technologies and programming languages that I have used in my professional projects are:\n\nMicrosoft Power BI with DAX for enterprise dashboards.\nMicrosoft Excel with Power Query for calculations and ETL.\nPython for solving discrete optimization problems, ETL, and dynamic visuals.\nVS Code as my main code editor.\nMicrosoft Sharepoint and Teams for collaboration.\ndraw.io for data- and entity-modelling.\n\nI also have experience with the following from courses and personal projects:\n\nT-SQL in Microsoft SQL Server and Azure SQL Services for learning about data warehouseing and enterprise analytics.\nPostgreSQL for learning SQL for local data warehousing.\nData Warehousing with Standard SQL in GCP’s BiqQuery\nSqlite for learning SQL for Data Science\nR-language data cleaning, transformations, and visualizations.\nTableau for creating visuals in dashboards.\nPySpark for learning real-time enterprise analytics.\n\nI’m currently living in Gothenburg Sweden and I’ve previously lived in\n\nMalaga, Spain\nKalmar, Sweden\nTokyo, Japan\nLondon, England\nHouston, USA\n\n\n\n\n\n\n\n\n\n\n\n\nTjörn, Sweden\n\n\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa\n\n\n\n\n\n\n\nOn the Tjörn island"
  },
  {
    "objectID": "PortfolioWebsite/about.html#pictures",
    "href": "PortfolioWebsite/about.html#pictures",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Tjörn, Sweden\n\n\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa\n\n\n\n\n\n\n\nOn the Tjörn island"
  },
  {
    "objectID": "PortfolioWebsite/projects/Eliminate/eliminate.html",
    "href": "PortfolioWebsite/projects/Eliminate/eliminate.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "PortfolioWebsite/projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "href": "PortfolioWebsite/projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "href": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?\n\n\n\n\nIn this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses. \n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.  #### Saving to historical exchange rate to datalake (tbc)"
  },
  {
    "objectID": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "href": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?"
  },
  {
    "objectID": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "href": "PortfolioWebsite/projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "In this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses. \n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.  #### Saving to historical exchange rate to datalake (tbc)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I have over six year’s experience in sustainability management consulting in the product development & purchasing departments of a multi-national manufacturing conglomerate. In parallel, I have data analytics/science experience from several EU-funded research projects within electrification and circular economy. From my formal education in B.Sc chemical engineering (with engineering physics), and a M.Sc. applied physics I have a solid understanding of applied mathematical modelling and statistics.\nI’m a certified Power BI Data Analyst associate and I’m also certified in Azure cloud and Azure AI fundamentals. I’m also well underway to become certified in end-to-end enterprise analytics as a Fabric Analytics Engineer associate.\n\n\n“Innovation distinguishes between a leader and a follower.” - Steve Jobs\nSome of my favourite technologies and programming languages that I have used in my professional projects are:\n\nMicrosoft Power BI with DAX for enterprise dashboards.\nMicrosoft Excel with Power Query for calculations and ETL.\nPython for solving discrete optimization problems, ETL, and dynamic visuals.\nVS Code as my main code editor.\nMicrosoft Sharepoint and Teams for collaboration.\ndraw.io for data- and entity-modelling.\n\nI also have experience with the following from courses and personal projects:\n\nT-SQL in Microsoft SQL Server and Azure SQL Services for learning about data warehouseing and enterprise analytics.\nPostgreSQL for learning SQL for local data warehousing.\nData Warehousing with Standard SQL in GCP’s BiqQuery\nSqlite for learning SQL for Data Science\nR-language data cleaning, transformations, and visualizations.\nTableau for creating visuals in dashboards.\nPySpark for learning real-time enterprise analytics.\n\nI’m currently living in Gothenburg Sweden and I’ve previously lived in\n\nMalaga, Spain\nKalmar, Sweden\nTokyo, Japan\nLondon, England\nHouston, USA\n\n\n\n\n\n\nTjörn, Sweden\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa"
  },
  {
    "objectID": "about.html#pictures",
    "href": "about.html#pictures",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Tjörn, Sweden\n\n\n\n\n\nEating churros and hot chocolate in Barcelona, Spain\n\n\n\n\n\nVisiting Kuusakoski dismantling facilities in Skellefteå, Sweden\n\n\n\n\n\nVisiting Northvolt on a rainy day in Skellefteå, Sweden\n\n\n\n\n\nVisiting Tabletop Mountain in Cape Town, South Africa"
  },
  {
    "objectID": "projects/Eliminate/eliminate.html",
    "href": "projects/Eliminate/eliminate.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "href": "projects/Eliminate/eliminate.html#optimizing-european-battery-recycling-through-a-data-driven-location-and-technology-analysis",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Github Repo, EU-funded Project Report, and Project Page\nTechnologies used: Python (Pandas, Plotly, PuLP) & Power BI.\nBusiness Problem: The European Union aims to scale battery recycling to meet growing demand from the electric vehicle market while reducing carbon emissions. The challenge is determining optimal locations and selecting the right recycling technologies to close Europe’s recycling capacity gap of 98,667 annual tonnes of battery cells scrap expected from 2026 onward.\nSolution: I developed an optimization model using geographic data and material flows to determine the best locations for recycling facilities. This model incorporates facility costs, CO2 emissions, and transportation costs, providing a roadmap for efficient and sustainable battery recycling in Europe.\n\n\nSummary: This project involved scraping and processing market data to optimize facility locations and technologies. Tools like Python and Power BI were used for data analysis, modeling, and visualization.\nUsed bottom-up data scraping with open-data on battery bill-of-materials to linearly extrapolate and estimate battery volumes and geographical coordinates for each battery actor, addressing the lack of comprehensive market data.\nCollaborated with cost analyst and LCA expert to integrate technoeconomic assessment and CO2-emissions on a country-basis.\nProcessed data and created an optimization model in Python with Pandas and PuLP packages. Created dynamic GIS and Sankey visualization using Python’s Plotly package. Created dynamic Power BI dashboard with Power BI.\n\n\n\nKey Findings:\n\nRecommended a single H2SO4 facility in Germany as the most cost-effective solution based on current market projections and transportation cost estimates.\nCompetitive landscape: High recycler concentration in Central Europe and a second cluster with fewer recyclers in Northern Europe.\nDashboard enhances decision-making by identifying competitors and capacity projections.\n\nPotential supply (collection points) and demand (battery producer locations) were mapped with their maximum processing volumes. The data on how other networks of recyclers in the region are unknown and thus the optimization would be incomplete without additional insights. The choice was made to scale the facility’s processing volumes to Europe’s projected unrecycled battery scrap in 2026 and find the optimal facility location disregarding the other networks. The optimization results in conjunction with a dashboard showing the projected processing volumes of other recyclers (competitors) over time gave an overal picture of suitable facility locations.\nBelow are projected market for supply- and demand-nodes for lithium-ion battery recyclers in Europe for 2030. The optimal cost and CO2-emissions were then calculated for combinations of supply- and demand-points. Different countries have different costs and CO2-emissions. The transportation costs were varied from between a low and high estimate based on usual costs and cost estimates for hazardous waste, as it was still not decided by the EU how battery waste would be classified. This tradeoff was done to focus on monetary costs.\n\n\n\nFigure 1: Projected battery recycler supply and demand nodes.\n\nNext are the optimal networks based on cost for the two transport cost estimates. For all technologies, regardless of transport estimate, the optimal number of facilities was one. The H2SO4 technology had the absolute lowest cost, with a network concentrated in Germany. A lower transport cost moved the optimzal network for MSA and H2SO4-NMC technologies to Northern Europe.\n\n Figure 2: Optimal facility locations based on cost.\n\nThe Power BI dashboard gives users the an additional perspective to the best location for the facility, by breaking down competitors by volumes processed for the years 2022, 2026, and 2030. Central Europe has a high concentration of recyclers opening up, meaning competition in the area will be fierce. Clicking the circles gives the company name and volumes processed, providing valuable information to determine who will likely be the biggest competitors for adding a new facility to the European region.\n\n\nFigure 3: Competitor dashboard to complement optimization results."
  },
  {
    "objectID": "other_projects/project-1/project-1.html",
    "href": "other_projects/project-1/project-1.html",
    "title": "Project 1",
    "section": "",
    "text": "This project explores the relationship between sepal length and petal length in iris flowers.\nLink to Project Repository [invalid URL removed]"
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?\nSolution: By integrating diverse datasets within Microsoft Fabric, I created a dynamic Power BI dashboard that offers a comprehensive view of the EU’s critical raw materials. This dashboard allows businesses to assess supply chain risks by not only considering economic importance and supply risk but also incorporating real-time market data and environmental performance indicators. This enables more informed, proactive decision-making to ensure supply chain resilience.\n\n\n\nIn this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses.\n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.\n\n\n\nI created a datalake to which I import the streaming exchange rate data. This allows for historical analysis of exchange rate fluctuations and their impact on material costs.\n\n\n\n\n\nThe final result is a Power BI dashboard that provides a comprehensive and interactive view of the EU’s critical raw materials. The dashboard allows users to:\n\nExplore Critical Materials: Interactively filter and view data for each critical raw material, including its economic importance, supply risk, and major suppliers.\nAnalyze Supply Chain Risks: Assess the risk associated with each material by drilling down into the various indicators that contribute to the overall risk score.\nMonitor Market Trends: Track the production of key industries in the EU and analyze how it relates to the demand for critical raw materials.\nEvaluate Environmental Impact: Compare the environmental performance of different supplier countries, providing a more holistic view of supply chain sustainability.\nGet Real-Time Exchange Rates: For businesses operating in different currencies, the dashboard provides real-time exchange rates to accurately assess costs.\n\nBy combining these different data sources into a single, easy-to-use dashboard, this project provides a powerful tool for European businesses to navigate the complexities of the global raw materials market and build more resilient and sustainable supply chains."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#executive-summary",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Project and Data Overview: An end-to-end data analytics project in Microsoft Fabric (a SaaS data platform) using publicly available statistics data from pdfs and APIs. The aim is to enhance existing ad-hoc analyses found in pdf-reports published by the European Commision by connecting to new data sources, including:\n\nvarious material, country, and region-level data (from the EU Commissions pdf reports),\nmarket-sector-level historical and updated EU production indicator in Euro (from the Eurostat API),\nlive exchange rate data (from fixer.io API), and\ncountry-level environmental impact indicators (from the Yale Environmental Performance Index, EPI).\n\nBusiness Question: How can a European business use the EU list of Critical Raw Materials (EU CRM) with additional metrics to mitigate supply chain risks?\nSolution: By integrating diverse datasets within Microsoft Fabric, I created a dynamic Power BI dashboard that offers a comprehensive view of the EU’s critical raw materials. This dashboard allows businesses to assess supply chain risks by not only considering economic importance and supply risk but also incorporating real-time market data and environmental performance indicators. This enables more informed, proactive decision-making to ensure supply chain resilience."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#project-walkthrough",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "In this project I use Microsoft Fabric to create dynamic custom visuals based on static data and reports from the European Commission’s list of Critical Raw Materials (EU CRM). I also use annually updated Eurostat datasets to compare with the every-third-year updated EU CRM list datasets.\nThese two approaches enable additional and fresher insights to be gained from the EU CRM list, which is at the heart of the EU Critical Raw Material strategy and legislation (EU CRM Regulation).\n\n\n\n\n\n\nNote\n\n\n\nEU CRM measure and prioritize material resource use quantitatively and qualitatively through risks associated with each respective material, broken down into the main categories of:\n\nEconomic Importance\nSupply Risk\n\nClassical risk theory is used to calculate the overall risk for each material, based on the main categories which are, in turn, calculated using combinations of various indicators. See the EU CRM methodology document for how the indicators form the main categories.\n\n\n\n\nTo start I open up a template for ‘Basic data analytics’ in Data Factory and name the workspace ‘CRM Learning Project’, which gives me just the necessary workloads I need for my analysis, namely:\n\nData Factory for orchestrating\nData Warehouse for importing structured data\nReal-Time Intelligence to test out live price data, and\nPower BI for visualizations.\n\n\n\n\nCreate a new Fabric workspace in Data Factory using template for basic data analytics\n\n\n\n\nI imported the main data from pdfs on the EU Commission’s site for CRM. I initially found the data to be almost impossible to clean using the Dataflow Gen2 connectors. There were many mismatched columns and cell values or critical errors in the datafram structures. I chose to publish my own cleaned datasets (using Power Query and manual cleaning) in a public github repository. This had several positives, including:\n\nthe simplification of data ingestion to Data Factory,\nenabling other analysts and developers to use the datasets and collaborate,\nenabling transparency into the data cleaning steps,\nscalability and tracking of future updates.\n\n\n\n\nGithub repo screenshot EU-Critical-Raw-Materials-CRM-Reports-Datasets\n\n\nTo import to Data Factory, I create a Dataflow Gen2 connect to each csv file using the Web API connector and enter the github repository csv-file URL and set ‘Authentication kind’ to Anonymous since the data is public.\n\n\n\n\n\n\nNote\n\n\n\nTo link to the csv files I used the direct Github link address structure: https://raw.githubusercontent.com/&lt;username&gt;/&lt;repository&gt;/&lt;branch&gt;/&lt;filepath&gt;/&lt;filename&gt;.csv\n\n\nDuring import I performed some simpler cleaning:\n\nIn the ‘Share’-column I removed rows with the value ‘&lt;1%’.\nIn the ‘Share’-column I\n\nremoved the percent signs with the ‘Replace values’ function,\ndivided the values by 100,\nchanged the datatype to ‘Whole Number’ temporarily so i can perform standard mathematical operations with the function under the ‘Transform’ tab, and\nchanged the datatype to percent.\n\nI replaced ‘-’ values with null using the ‘Replace values’ function.\nI changed datatypes of decimals to “Decimal Number”\nI replaced ‘PGMs’, ‘HREEs’, and ‘LREEs’ values with ‘PGM’, ‘HREE’, ‘LREE’, respectively.\n\n\n\n\nRemove rows that contain ‘&lt;1%’\n\n\n\n\n\nEurostat contains a wealth of information for the EU on the country-level, but for this analysis I used it to connect to a specific respective dataset in the static EU CRM report for comparison. The methdology report states that ‘NACE v2’ categories indicating ‘production in industry’ to describe each country’s demands.\nImport by going to the Eurostat database, select ‘Industry, trade and services’, and ‘Short-term business statistics’, and ‘Industry’, and ‘Production in Industry’. Then click on the ‘Production in industry - annual data’ dataset.\n\n\n\nClick through to access the production dataset in Eurostat.\n\n\nIn the next page, set the filters to only include the NACE rev. 2 activities C19-C31 based on the existing activities in the static EU Critical Raw Materials Data which I will later connect to in the data model. After the filtering, clicking ‘download’ will open up options. Clicking ‘Advanced settings’ opens up additional options from which choosing the ‘SDMX-ML Genergic Data (2.1)’ will give a link to which the Dataflow Gen2 connector can use the ‘SIS-CC SDMX’ API connector with. Finally, selecting only the relevant rows and setting the datatypes will yield the desired table for publishing in Data Factory.\nIn addition to changing data types, I added a custom column and used the code below to cut out the first three letters/numbers from the NACE column to ensure accurate connections to the model in Power BI later on.\nText.Start([NACE sector], 3)\n\n\n\nI wanted a use-case for Microsoft’s Real-Time Intelligence, so I found fixer.io which provides a free-tier api of hourly-updated exchange rates. From its API, I import streaming data to connect to Power BI’s data model later. The data I import is the exchange from Euros to Swedish Krona (SEK), to be used to translate the costs for Swedish businesses.\n\n\n\nYale EPI data is published annually in csv files. I uploaded the files to my github portfolio repository because they couldn’t be uploaded directly to Fabric as it wasn’t tied to a OneDrive for Business account. A simple Dataflow Gen2 connector using Web API was enough to ingest the data to the Data Factory.\n\n\n\n\n\n\nAll my tables except for the exchange rates are fed into the data warehouse I create in Data Factory.\nI use a Dataflow gen2 connector for the import step, and see an issue that several different country codes are being used for tables coming from different sources. To manage these discrepancies, I decide to translate each table’s countries to the ISO 3-letter standard. To do this I:\n\nImporting another table with ISO country codes\nthat I use to translate existing tables’ countries with.\n\nHere I also notice some characters that are not allowed, such as ‘ü’ in Türkye, which I fix in the original Dataflow gen2 connector.\n\n\n\nI created a datalake to which I import the streaming exchange rate data. This allows for historical analysis of exchange rate fluctuations and their impact on material costs."
  },
  {
    "objectID": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#results",
    "href": "projects/Fabric_CRM_Learning_Project/Fabric_CRM_Learning_Project.html#results",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "The final result is a Power BI dashboard that provides a comprehensive and interactive view of the EU’s critical raw materials. The dashboard allows users to:\n\nExplore Critical Materials: Interactively filter and view data for each critical raw material, including its economic importance, supply risk, and major suppliers.\nAnalyze Supply Chain Risks: Assess the risk associated with each material by drilling down into the various indicators that contribute to the overall risk score.\nMonitor Market Trends: Track the production of key industries in the EU and analyze how it relates to the demand for critical raw materials.\nEvaluate Environmental Impact: Compare the environmental performance of different supplier countries, providing a more holistic view of supply chain sustainability.\nGet Real-Time Exchange Rates: For businesses operating in different currencies, the dashboard provides real-time exchange rates to accurately assess costs.\n\nBy combining these different data sources into a single, easy-to-use dashboard, this project provides a powerful tool for European businesses to navigate the complexities of the global raw materials market and build more resilient and sustainable supply chains."
  },
  {
    "objectID": "projects/learning_projects.html",
    "href": "projects/learning_projects.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Here I share some of my learning projects that I’ve done to gain experience in SQL, cloud services, data warehousing, and more.\n\nLearning Projects\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nBelow is the script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I specialize in cloud and data analytics to optimize logistics, supply chains, and sustainability strategies. Contact me on LinkedIn\n\nProfessional Projects\n\nOptimizing European Battery Recycling Through a Data-Driven Location and Technology Analysis\nTechnologies used: Python, Power BI, Github.\n\n\n\nLearning Projects\n\nMicrosoft Fabric End-to-End Analytics on Critical Raw Materials\nTechnologies used: Microsoft Fabric, Data Factory, Power BI, Github.\n\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nA script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "PortfolioWebsite/other_projects/project-1/project-1.html",
    "href": "PortfolioWebsite/other_projects/project-1/project-1.html",
    "title": "Project 1",
    "section": "",
    "text": "This project explores the relationship between sepal length and petal length in iris flowers.\nLink to Project Repository [invalid URL removed]"
  },
  {
    "objectID": "PortfolioWebsite/projects/learning_projects.html",
    "href": "PortfolioWebsite/projects/learning_projects.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "Here I share some of my learning projects that I’ve done to gain experience in SQL, cloud services, data warehousing, and more.\n\nLearning Projects\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nBelow is the script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  },
  {
    "objectID": "PortfolioWebsite/index.html",
    "href": "PortfolioWebsite/index.html",
    "title": "Erik Emilsson’s Portfolio",
    "section": "",
    "text": "I specialize in cloud and data analytics to optimize logistics, supply chains, and sustainability strategies. Contact me on LinkedIn\n\nProfessional Projects\n\nOptimizing European Battery Recycling Through a Data-Driven Location and Technology Analysis\nTechnologies used: Python, Power BI, Github.\n\n\n\nLearning Projects\n\nMicrosoft Fabric End-to-End Analytics on Critical Raw Materials\nTechnologies used: Microsoft Fabric, Data Factory, Power BI, Github.\n\n\nBike-Share Business Analsis for Adopting More Members\nTechnologies used: BigQuery Data Warehouse, R-language.\nA script file for the data cleaning, exploration, and visualizations I created in an R-Notebook. Prior to loading the data into the notebook, the data was loaded and transformed in BigQuery’s sandbox environment using SQL queries in PostgreSQL flavor."
  }
]